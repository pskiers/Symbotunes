import torch


class FolkTokenizer:
    tokens = [
        "(2",
        "(3",
        "(4",
        "(5",
        "(6",
        "(7",
        "(9",
        "/2",
        "/2<",
        "/2>",
        "/3",
        "/4",
        "/8",
        "12",
        "16",
        "2",
        "2<",
        "2>",
        "3",
        "3/2",
        "3/4",
        "4",
        "4>",
        "5",
        "5/2",
        "6",
        "7",
        "7/2",
        "8",
        "9",
        ":|",
        "<",
        "=A",
        "=A,",
        "=B",
        "=B,",
        "=C",
        "=C,",
        "=D",
        "=E",
        "=E,",
        "=F",
        "=F,",
        "=G",
        "=G,",
        "=a",
        "=b",
        "=c",
        "=c'",
        "=d",
        "=e",
        "=e'",
        "=f",
        "=f'",
        "=g",
        ">",
        "A",
        "A,",
        "B",
        "B,",
        "C",
        "C,",
        "D",
        "D,",
        "E",
        "E,",
        "F",
        "F,",
        "G",
        "G,",
        "K:Cdor",
        "K:Cmaj",
        "K:Cmin",
        "K:Cmix",
        "M:12/8",
        "M:2/4",
        "M:3/2",
        "M:3/4",
        "M:4/4",
        "M:6/8",
        "M:9/8",
        "[",
        "]",
        "^A",
        "^A,",
        "^C",
        "^C,",
        "^D",
        "^F",
        "^F,",
        "^G",
        "^G,",
        "^a",
        "^c",
        "^c'",
        "^d",
        "^f",
        "^f'",
        "^g",
        "_A",
        "_A,",
        "_B",
        "_B,",
        "_C",
        "_D",
        "_E",
        "_E,",
        "_G",
        "_a",
        "_b",
        "_c",
        "_d",
        "_d'",
        "_e",
        "_e'",
        "_g",
        "a",
        "a'",
        "b",
        "b'",
        "c",
        "c'",
        "d",
        "d'",
        "e",
        "e'",
        "f",
        "f'",
        "g",
        "g'",
        "z",
        "|",
        "|1",
        "|2",
        "|:",
    ]

    def __init__(self) -> None:
        self.token2idx = {token: idx for idx, token in enumerate(self.tokens)}
        self.idx2token = {idx: token for idx, token in enumerate(self.tokens)}
        self.start_token = len(self.tokens)
        self.end_token = len(self.tokens) + 1

    def inverse_transform(self, data: torch.Tensor, start_token_present=True, end_token_present=True) -> list[str]:
        data = data[1:] if start_token_present else data
        data = data[:-1] if end_token_present else data
        return [self.idx2token[int(token)] for token in data]

    def __call__(self, data: str):
        tunes = [self.start_token]
        tunes += [self.token2idx[token] for token in data.split()]
        tunes.append(self.end_token)
        return torch.tensor(tunes)
